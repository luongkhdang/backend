version: "3.8"

services:
  # Reader DB - PostgreSQL database with pgvector for storing transferred articles
  postgres:
    image: ankane/pgvector:latest
    container_name: reader-db
    environment:
      POSTGRES_DB: reader_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5433:5432" # Map to port 5433 externally to avoid conflicts with newsdb
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - reader_network

  # Article Transfer Service - Transfers articles from news-api to reader-db
  article-transfer:
    build:
      context: .
      dockerfile: Dockerfile
    image: article-transfer-app:latest
    container_name: article-transfer
    restart: "no"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Reader DB connection
      READER_DB_HOST: postgres # Use container name within Docker network
      READER_DB_PORT: 5432 # Internal port in Docker network
      READER_DB_NAME: reader_db
      READER_DB_USER: postgres
      READER_DB_PASSWORD: postgres
      # News API connection - just use service name, which will be resolved by Docker DNS
      NEWS_API_BASE_URL: http://news-api:8000
      # Processing settings
      MAX_WORKERS: 4 # Number of parallel workers for processing
      DB_UPDATE_BATCH_SIZE: 20 # Batch size for DB inserts in Step 1.4
      CHECKPOINT_INTERVAL_SECONDS: 60 # Interval for DB updates

      # --- Gemini API Key ---
      GEMINI_API_KEY: ${GEMINI_API_KEY:-YOUR_API_KEY_HERE}

      # --- Gemini Embedding Model ---
      GEMINI_EMBEDDING_MODEL_ID: ${GEMINI_EMBEDDING_MODEL_ID:-models/text-embedding-004}
      GEMINI_EMBEDDING_RPM: ${GEMINI_EMBEDDING_RPM:-1500} # Default RPM
      GEMINI_EMBEDDING_INPUT_TOKEN_LIMIT: ${GEMINI_EMBEDDING_INPUT_TOKEN_LIMIT:-2048} # For warnings
      GEMINI_EMBEDDING_TASK_TYPE: ${GEMINI_EMBEDDING_TASK_TYPE:-CLUSTERING}
      GEMINI_EMBEDDING_OUTPUT_DIMENSION: ${GEMINI_EMBEDDING_OUTPUT_DIMENSION:-768}
      # Note: TPM/RPD not typically limited for embedding models in the same way

      # --- Gemini Generation Models (Current Use) ---
      # Shared Limits
      GEMINI_GEN_INPUT_TOKEN_LIMIT: ${GEMINI_GEN_INPUT_TOKEN_LIMIT:-1048576}

      # Model: gemini-2.0-flash-exp (Now highest priority)
      GEMINI_FLASH_EXP_MODEL_ID: ${GEMINI_FLASH_EXP_MODEL_ID:-gemini-2.0-flash-exp}
      GEMINI_FLASH_EXP_RPM: ${GEMINI_FLASH_EXP_RPM:-10} # Using thinking-exp RPM as proxy
      GEMINI_FLASH_EXP_TPM: ${GEMINI_FLASH_EXP_TPM:-4000000} # Using thinking-exp TPM as proxy
      GEMINI_FLASH_EXP_RPD: ${GEMINI_FLASH_EXP_RPD:-1500} # Using thinking-exp RPD as proxy
      GEMINI_FLASH_EXP_THINKING: ${GEMINI_FLASH_EXP_THINKING:-true} # Assuming true like thinking-exp
      GEMINI_FLASH_EXP_GROUNDING: ${GEMINI_FLASH_EXP_GROUNDING:-true} # Assuming true like thinking-exp

      # Model: gemini-2.0-flash (Now medium priority)
      GEMINI_FLASH_MODEL_ID: ${GEMINI_FLASH_MODEL_ID:-gemini-2.0-flash}
      GEMINI_FLASH_RPM: ${GEMINI_FLASH_RPM:-15}
      GEMINI_FLASH_TPM: ${GEMINI_FLASH_TPM:-1000000}
      GEMINI_FLASH_RPD: ${GEMINI_FLASH_RPD:-1500}
      GEMINI_FLASH_THINKING: ${GEMINI_FLASH_THINKING:-true}
      GEMINI_FLASH_GROUNDING: ${GEMINI_FLASH_GROUNDING:-true}

      # Model: gemini-2.0-flash-lite
      GEMINI_FLASH_LITE_MODEL_ID: ${GEMINI_FLASH_LITE_MODEL_ID:-gemini-2.0-flash-lite}
      GEMINI_FLASH_LITE_RPM: ${GEMINI_FLASH_LITE_RPM:-30}
      GEMINI_FLASH_LITE_TPM: ${GEMINI_FLASH_LITE_TPM:-1000000}
      GEMINI_FLASH_LITE_RPD: ${GEMINI_FLASH_LITE_RPD:-1500}
      GEMINI_FLASH_LITE_THINKING: ${GEMINI_FLASH_LITE_THINKING:-false}
      GEMINI_FLASH_LITE_GROUNDING: ${GEMINI_FLASH_LITE_GROUNDING:-false}

      # --- Gemini Generation Models (Future Use) ---
      # Model: gemini-2.5-flash-preview-04-17
      GEMINI_PREVIEW_MODEL_ID: ${GEMINI_PREVIEW_MODEL_ID:-gemini-2.5-flash-preview-04-17}
      GEMINI_PREVIEW_RPM: ${GEMINI_PREVIEW_RPM:-10}
      GEMINI_PREVIEW_TPM: ${GEMINI_PREVIEW_TPM:-250000}
      GEMINI_PREVIEW_RPD: ${GEMINI_PREVIEW_RPD:-500}
      GEMINI_PREVIEW_THINKING: ${GEMINI_PREVIEW_THINKING:-true}
      GEMINI_PREVIEW_GROUNDING: ${GEMINI_PREVIEW_GROUNDING:-true}

      # --- Model Selection Preferences ---
      GEMINI_MODEL_PREF_1: ${GEMINI_MODEL_PREF_1:-gemini-2.0-flash-exp} # NEW Highest priority
      GEMINI_MODEL_PREF_2: ${GEMINI_MODEL_PREF_2:-gemini-2.0-flash} # Medium priority
      GEMINI_MODEL_PREF_3: ${GEMINI_MODEL_PREF_3:-gemini-2.0-flash-lite} # Lowest priority
      GEMINI_FALLBACK_MODEL_ID: ${GEMINI_FALLBACK_MODEL_ID:-gemini-2.0-flash} # Fallback

      # --- Other Settings (Rate Limiting & General) ---
      GEMINI_MAX_WAIT_SECONDS: ${GEMINI_MAX_WAIT_SECONDS:-40}
      # Python path for Google imports
      PYTHONPATH: /app
      # Debug flag for imports
      PYTHONDEBUG: "1"
      # Step Execution Control
      RUN_PHASE_1_STEPS: ${RUN_PHASE_1_STEPS:-false} # Master switch for steps 1-4
      RUN_STEP1: ${RUN_STEP1:-true} # Controls Step 1
      RUN_STEP2: ${RUN_STEP2:-true} # Controls Step 2 (Clustering)
      RUN_STEP3: ${RUN_STEP3:-true} # Controls Step 3 (Entity Extraction)
      RUN_STEP4: ${RUN_STEP4:-true} # Controls Step 4 (Data Export)
      RUN_PHASE_2_STEPS: ${RUN_PHASE_2_STEPS:-true} # Master switch for step 5 (Essay Gen)
      RUN_STEP5: ${RUN_STEP5:-true} # Controls Step 5 (RAG Essay Generation)
      # Local NLP Settings (Step 1.7 - Now always runs if Step 1 runs)
      LOCAL_NLP_MODEL: ${LOCAL_NLP_MODEL:-"facebook/bart-large-cnn"}
      MAX_SUMMARY_TOKENS: ${MAX_SUMMARY_TOKENS:-512}
      MIN_SUMMARY_TOKENS: ${MIN_SUMMARY_TOKENS:-150}
      # Chunking parameters for Step 1.7
      ARTICLE_TOKEN_CHUNK_THRESHOLD: ${ARTICLE_TOKEN_CHUNK_THRESHOLD:-2000}
      TARGET_CHUNK_TOKEN_SIZE: ${TARGET_CHUNK_TOKEN_SIZE:-1000}
      CHUNK_MAX_TOKENS: ${CHUNK_MAX_TOKENS:-300}
      CHUNK_MIN_TOKENS: ${CHUNK_MIN_TOKENS:-75}
      # Step 2: Clustering settings (Switches removed, settings remain)
      MIN_CLUSTER_SIZE: ${MIN_CLUSTER_SIZE:-10}
      HOT_CLUSTER_THRESHOLD: ${HOT_CLUSTER_THRESHOLD:-20}
      MAX_CLUSTERS_TO_INTERPRET: ${MAX_CLUSTERS_TO_INTERPRET:-10}
      CLUSTER_SAMPLE_SIZE: ${CLUSTER_SAMPLE_SIZE:-10}
      # Step 3: Entity Extraction settings (Switches removed, settings remain)
      ENTITY_EXTRACTION_BATCH_SIZE: ${ENTITY_EXTRACTION_BATCH_SIZE:-10}
      ENTITY_MAX_PRIORITY_ARTICLES: ${ENTITY_MAX_PRIORITY_ARTICLES:-100}
      # Step 4: Data Export settings
      EMERGENCY_FALLBACK_WAIT_SECONDS: ${EMERGENCY_FALLBACK_WAIT_SECONDS:-120}
      # Hotness calculation factors (Switches removed, settings remain)
      RECENCY_DAYS: ${RECENCY_DAYS:-3}
      CORE_TOPIC_KEYWORDS: ${CORE_TOPIC_KEYWORDS:-"china,us,united states,vietnam,europe,germany,war,trade,exports,tariffs,geopolitics,geopolitical,political economy,influence,lobbying,narrative,framing,disinformation,misinformation,ai,artificial intelligence,election,campaign,pentagon,defense,state department,diplomacy,itc,international trade commission"}
      W_SIZE: ${W_SIZE:-0.15}
      W_RECENCY: ${W_RECENCY:-0.30}
      W_INFLUENCE: ${W_INFLUENCE:-0.30}
      W_RELEVANCE: ${W_RELEVANCE:-0.25}
      TARGET_HOT_CLUSTERS: ${TARGET_HOT_CLUSTERS:-8}
      # Debugging options
      LOG_LEVEL: DEBUG
      DEBUG_GEMINI_CLIENT: "true"
      # Polling interval
      POLLING_INTERVAL: 300
      WAIT_SECONDS: 10
      # Use skip-network-check to bypass Docker CLI checks
      FORCE_CONTINUE_ON_NETWORK_ERROR: "true"
    # Add the skip-network-check flag to avoid Docker introspection
    command: ["python", "src/main.py", "--workers", "8"]
    # Add GPU resource request
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # Request access to 1 NVIDIA GPU
              capabilities: [gpu, compute, utility]
    volumes:
      - ./src:/app/src # Mount source code for easier development
      - ./logs:/app/logs # Mount logs directory for error reports
      - huggingface_cache:/root/.cache/huggingface # Mount cache volume for HuggingFace models
      - ./src/output:/app/src/output # Mount output directory for persisting exported data
    networks:
      - reader_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import requests; requests.get('http://postgres:5432')",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # pgAdmin - Web interface for database management
  pgadmin:
    image: dpage/pgadmin4
    container_name: reader-pgadmin
    depends_on:
      postgres:
        condition: service_started
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@reader.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5051:80"
    restart: unless-stopped
    volumes:
      - pgadmin_data:/var/lib/pgadmin # Mount named volume for persistent data
    networks:
      - reader_network

  # Backend service for database setup and management
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: reader-backend
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      READER_DB_HOST: postgres # Use container name within Docker network
      READER_DB_PORT: 5432 # Internal port in Docker network
      READER_DB_NAME: reader_db
      READER_DB_USER: postgres
      READER_DB_PASSWORD: postgres
      # Step 3: Entity Extraction settings (Keep these as db_setup might initialize related tables/weights)
      RUN_STEP3: ${RUN_STEP3:-true}
      ENTITY_EXTRACTION_BATCH_SIZE: ${ENTITY_EXTRACTION_BATCH_SIZE:-10}
      ENTITY_MAX_PRIORITY_ARTICLES: ${ENTITY_MAX_PRIORITY_ARTICLES:-100}
      CALCULATE_INFLUENCE_SCORES: ${CALCULATE_INFLUENCE_SCORES:-true}
      # Python path for Google imports
      PYTHONPATH: /app
      # Wait settings
      WAIT_FOR_DB: "true"
      WAIT_SECONDS: "10"
    volumes:
      - ./src:/app/src # Mount source code for easier development
    restart: unless-stopped
    networks:
      - reader_network
    command: ["python", "src/database/db_setup.py"]

# Define Docker networks
networks:
  reader_network:
    # This file now manages the creation of reader_network
    driver: bridge
    # Optionally specify the name if desired, otherwise it defaults based on directory
    name: reader_network

# Define persistent volumes
volumes:
  postgres_data: # Persistent volume for reader-db data
  huggingface_cache: # Persistent volume for Hugging Face models/cache
  pgadmin_data: # Persistent volume for pgAdmin configuration
