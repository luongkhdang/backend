version: "3.8"

services:
  # Reader DB - PostgreSQL database with pgvector for storing transferred articles
  postgres:
    image: ankane/pgvector:latest
    container_name: reader-db
    environment:
      POSTGRES_DB: reader_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5433:5432" # Map to port 5433 externally to avoid conflicts with news-db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - reader_network

  # Article Transfer Service - Transfers articles from news-api to reader-db
  article-transfer:
    build:
      context: .
      dockerfile: Dockerfile
    image: article-transfer-app:latest
    container_name: article-transfer
    restart: "no"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Reader DB connection
      READER_DB_HOST: postgres # Use container name within Docker network
      READER_DB_PORT: 5432 # Internal port in Docker network
      READER_DB_NAME: reader_db
      READER_DB_USER: postgres
      READER_DB_PASSWORD: postgres
      # News API connection - just use service name, which will be resolved by Docker DNS
      NEWS_API_BASE_URL: http://news-api:8000
      # Processing settings
      MAX_WORKERS: 4 # Number of parallel workers for processing
      DB_UPDATE_BATCH_SIZE: 20 # Batch size for DB inserts in Step 1.4
      CHECKPOINT_INTERVAL_SECONDS: 60 # Interval for DB updates
      # Gemini API settings
      GEMINI_API_KEY: ${GEMINI_API_KEY:-YOUR_API_KEY_HERE}
      GEMINI_EMBEDDING_MODEL: ${GEMINI_EMBEDDING_MODEL:-models/text-embedding-004}
      GEMINI_EMBEDDING_TASK_TYPE: ${GEMINI_EMBEDDING_TASK_TYPE:-CLUSTERING}
      GEMINI_EMBEDDING_INPUT_TOKEN_LIMIT: ${GEMINI_EMBEDDING_INPUT_TOKEN_LIMIT:-2048}
      GEMINI_EMBEDDING_OUTPUT_DIMENSION: ${GEMINI_EMBEDDING_OUTPUT_DIMENSION:-768}
      GEMINI_EMBEDDING_RATE_LIMIT_PER_MINUTE: ${GEMINI_EMBEDDING_RATE_LIMIT_PER_MINUTE:-1500}

      # --- Gemini Generation Models --- #
      # Shared limits
      GEMINI_GEN_INPUT_TOKEN_LIMIT: ${GEMINI_GEN_INPUT_TOKEN_LIMIT:-1048576}
      GEMINI_GEN_OUTPUT_TOKEN_LIMIT: ${GEMINI_GEN_OUTPUT_TOKEN_LIMIT:-8192}
      # Model: gemini-2.0-flash (Default)
      GEMINI_FLASH_MODEL: ${GEMINI_FLASH_MODEL:-models/gemini-2.0-flash}
      GEMINI_FLASH_RPM: ${GEMINI_FLASH_RPM:-15}
      GEMINI_FLASH_TPM: ${GEMINI_FLASH_TPM:-1000000}
      GEMINI_FLASH_RPD: ${GEMINI_FLASH_RPD:-1500}
      # Model: gemini-2.0-flash-thinking-exp-01-21
      GEMINI_FLASH_THINKING_MODEL: ${GEMINI_FLASH_THINKING_MODEL:-models/gemini-2.0-flash-thinking-exp-01-21}
      GEMINI_FLASH_THINKING_RPM: ${GEMINI_FLASH_THINKING_RPM:-10}
      GEMINI_FLASH_THINKING_TPM: ${GEMINI_FLASH_THINKING_TPM:-4000000}
      GEMINI_FLASH_THINKING_RPD: ${GEMINI_FLASH_THINKING_RPD:-1500}
      # Model: gemini-2.0-flash-exp
      GEMINI_FLASH_EXP_MODEL: ${GEMINI_FLASH_EXP_MODEL:-models/gemini-2.0-flash-exp}
      GEMINI_FLASH_EXP_RPM: ${GEMINI_FLASH_EXP_RPM:-10}
      GEMINI_FLASH_EXP_TPM: ${GEMINI_FLASH_EXP_TPM:-1000000}
      GEMINI_FLASH_EXP_RPD: ${GEMINI_FLASH_EXP_RPD:-1500}
      # Model: gemini-2.0-flash-lite (Fallback)
      GEMINI_FLASH_LITE_MODEL: ${GEMINI_FLASH_LITE_MODEL:-models/gemini-2.0-flash-lite}
      GEMINI_FLASH_LITE_RPM: ${GEMINI_FLASH_LITE_RPM:-30}
      GEMINI_FLASH_LITE_TPM: ${GEMINI_FLASH_LITE_TPM:-1000000}
      GEMINI_FLASH_LITE_RPD: ${GEMINI_FLASH_LITE_RPD:-1500}

      # --- Other Settings --- #
      # Python path for Google imports
      PYTHONPATH: /app
      # Debug flag for imports
      PYTHONDEBUG: "1"
      # Local NLP Settings (Step 1.7)
      RUN_STEP_1_7: ${RUN_STEP_1_7:-true}
      LOCAL_NLP_MODEL: ${LOCAL_NLP_MODEL:-"facebook/bart-large-cnn"}
      MAX_SUMMARY_TOKENS: ${MAX_SUMMARY_TOKENS:-512}
      MIN_SUMMARY_TOKENS: ${MIN_SUMMARY_TOKENS:-150}
      # Chunking parameters for Step 1.7
      ARTICLE_TOKEN_CHUNK_THRESHOLD: ${ARTICLE_TOKEN_CHUNK_THRESHOLD:-2000}
      TARGET_CHUNK_TOKEN_SIZE: ${TARGET_CHUNK_TOKEN_SIZE:-1000}
      CHUNK_MAX_TOKENS: ${CHUNK_MAX_TOKENS:-300}
      CHUNK_MIN_TOKENS: ${CHUNK_MIN_TOKENS:-75}
      # Step 2: Clustering settings
      RUN_CLUSTERING_STEP: ${RUN_CLUSTERING_STEP:-true}
      MIN_CLUSTER_SIZE: ${MIN_CLUSTER_SIZE:-10}
      HOT_CLUSTER_THRESHOLD: ${HOT_CLUSTER_THRESHOLD:-20}
      INTERPRET_CLUSTERS: ${INTERPRET_CLUSTERS:-true}
      MAX_CLUSTERS_TO_INTERPRET: ${MAX_CLUSTERS_TO_INTERPRET:-10}
      CLUSTER_SAMPLE_SIZE: ${CLUSTER_SAMPLE_SIZE:-10}
      # Step 3: Entity Extraction settings
      RUN_STEP3: ${RUN_STEP3:-true}
      ENTITY_EXTRACTION_BATCH_SIZE: ${ENTITY_EXTRACTION_BATCH_SIZE:-10}
      ENTITY_MAX_PRIORITY_ARTICLES: ${ENTITY_MAX_PRIORITY_ARTICLES:-100}
      # Hotness calculation factors
      RECENCY_DAYS: ${RECENCY_DAYS:-3}
      CALCULATE_TOPIC_RELEVANCE: ${CALCULATE_TOPIC_RELEVANCE:-true}
      CORE_TOPIC_KEYWORDS: ${CORE_TOPIC_KEYWORDS:-"china,us,united states,vietnam,europe,germany,war,trade,exports,tariffs,geopolitics,geopolitical,political economy,influence,lobbying,narrative,framing,disinformation,misinformation,ai,artificial intelligence,election,campaign,pentagon,defense,state department,diplomacy,itc,international trade commission"}
      W_SIZE: ${W_SIZE:-0.15}
      W_RECENCY: ${W_RECENCY:-0.30}
      W_INFLUENCE: ${W_INFLUENCE:-0.30}
      W_RELEVANCE: ${W_RELEVANCE:-0.25}
      TARGET_HOT_CLUSTERS: ${TARGET_HOT_CLUSTERS:-8}
      # Debugging options
      LOG_LEVEL: INFO
      # Polling interval
      POLLING_INTERVAL: 300
      WAIT_SECONDS: 10
      # Use skip-network-check to bypass Docker CLI checks
      FORCE_CONTINUE_ON_NETWORK_ERROR: "true"
    # Add the skip-network-check flag to avoid Docker introspection
    command: ["python", "src/main.py", "--workers", "4"]
    # Add GPU resource request
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # Request access to 1 NVIDIA GPU
              capabilities: [gpu, compute, utility]
    volumes:
      - ./src:/app/src # Mount source code for easier development
      - ./logs:/app/logs # Mount logs directory for error reports
      - huggingface_cache:/root/.cache/huggingface # Mount cache volume for HuggingFace models
    networks:
      - reader_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import requests; requests.get('http://postgres:5432')",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # pgAdmin - Web interface for database management
  pgadmin:
    image: dpage/pgadmin4
    container_name: reader-pgadmin
    depends_on:
      postgres:
        condition: service_started
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@reader.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5051:80"
    restart: unless-stopped
    networks:
      - reader_network

  # Backend service for database setup and management
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: reader-backend
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      READER_DB_HOST: postgres # Use container name within Docker network
      READER_DB_PORT: 5432 # Internal port in Docker network
      READER_DB_NAME: reader_db
      READER_DB_USER: postgres
      READER_DB_PASSWORD: postgres
      # Gemini API settings (also pass to backend if needed for future use)
      GEMINI_API_KEY: ${GEMINI_API_KEY:-YOUR_API_KEY_HERE}
      GEMINI_EMBEDDING_MODEL: ${GEMINI_EMBEDDING_MODEL:-models/text-embedding-004}
      GEMINI_EMBEDDING_RATE_LIMIT_PER_MINUTE: ${GEMINI_EMBEDDING_RATE_LIMIT_PER_MINUTE:-1500}
      # Generation Models (Main and Fallback might be useful)
      GEMINI_FLASH_MODEL: ${GEMINI_FLASH_MODEL:-models/gemini-2.0-flash}
      GEMINI_FLASH_RPM: ${GEMINI_FLASH_RPM:-15}
      GEMINI_FLASH_LITE_MODEL: ${GEMINI_FLASH_LITE_MODEL:-models/gemini-2.0-flash-lite}
      GEMINI_FLASH_LITE_RPM: ${GEMINI_FLASH_LITE_RPM:-30}
      # Step 3: Entity Extraction settings
      RUN_STEP3: ${RUN_STEP3:-true}
      ENTITY_EXTRACTION_BATCH_SIZE: ${ENTITY_EXTRACTION_BATCH_SIZE:-10}
      ENTITY_MAX_PRIORITY_ARTICLES: ${ENTITY_MAX_PRIORITY_ARTICLES:-100}
      # Python path for Google imports
      PYTHONPATH: /app
      # Wait settings
      WAIT_FOR_DB: "true"
      WAIT_SECONDS: "10"
    volumes:
      - ./src:/app/src # Mount source code for easier development
    restart: unless-stopped
    networks:
      - reader_network
    command: ["python", "src/database/db_setup.py"]

# Define Docker networks
networks:
  reader_network:
    # This file now manages the creation of reader_network
    driver: bridge
    # Optionally specify the name if desired, otherwise it defaults based on directory
    name: reader_network

# Define persistent volumes
volumes:
  postgres_data: # Persistent volume for reader-db data
  huggingface_cache: # Persistent volume for Hugging Face models/cache
